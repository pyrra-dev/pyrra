# Task 8.4.2: Root Cause Analysis - Regex Label Selector Behavior

**Task:** Root cause analysis for regex label selector behavior  
**Status:** ✅ Complete  
**Date:** 2025-10-14  
**Prerequisites:** Task 8.4.1 upstream comparison testing complete

---

## Executive Summary

Root cause analysis confirms that the observed behavior is **existing upstream Pyrra design**, not a regression. Two separate issues identified with clear architectural explanations.

### Issue #1: Grouping Creates Multiple SLO Instances

**Root Cause:** Intentional Pyrra design for per-label-value SLO tracking

**Architecture:**
- Pyrra processes `grouping` field to create separate SLO instances
- One SLO instance per unique value of grouped labels
- Recording rules use `sum by (label)` to maintain per-instance metrics
- Each instance appears as separate entry in UI

**Code Flow:**
1. SLO YAML parsed with `grouping: [handler]`
2. Pyrra queries Prometheus for unique handler values
3. Creates one SLO instance per handler value
4. Generates recording rules with `sum by (handler)`
5. UI displays each instance separately

**Is This a Bug?**
- **No** - This appears to be intentional design for per-endpoint tracking
- Allows users to track SLOs for individual endpoints
- Recording rules correctly scoped per-group
- Detail pages work as designed

**Confusion Factor:**
- Users expect: One YAML = One SLO
- Actual behavior: One YAML = Multiple SLOs
- Documentation doesn't clearly explain this behavior

---

### Issue #2: NaN in Burn Rate Columns

**Root Cause:** Prometheus returns empty results (not 0) for non-existent metrics

**Technical Explanation:**

When there are no errors in the measurement window:

```promql
# Step 1: Query for errors (none exist)
sum(rate(prometheus_http_requests_total{code=~"5..",handler=~"/api.*"}[3m]))
→ Returns: [] (empty vector, not 0)

# Step 2: Query for total requests (traffic exists)
sum(rate(prometheus_http_requests_total{handler=~"/api.*"}[3m]))
→ Returns: [0.5] (actual traffic)

# Step 3: Calculate burn rate (error rate)
[] / [0.5]
→ Returns: [] (empty, not 0)

# Step 4: UI displays empty as NaN
```

**Why Prometheus Returns Empty:**
- Prometheus doesn't create time series for metrics that don't exist
- When `code=~"5.."` matches nothing, the query returns empty
- Empty vector in division produces empty result (not 0)
- This is standard Prometheus behavior

**Why Availability/Budget Tiles Work:**
- Use different calculation method with `or vector(0)` fallback
- Example: `sum(errors or vector(0)) / sum(total)`
- Fallback ensures 0 instead of empty

**UI Handling:**
- Upstream UI doesn't handle empty results gracefully
- JavaScript interprets empty/undefined as NaN
- Feature branch likely fixes this with proper null checks

---

## Detailed Code Analysis

### Grouping Behavior

**Relevant Code Locations:**
- `slo/slo.go` - SLO object handling and grouping field processing
- `slo/rules.go` - Recording rule generation with `sum by (labels)`
- `kubernetes/controllers/` - SLO instantiation from CRDs

**Recording Rule Generation Pattern:**

```go
// Without grouping
expr := fmt.Sprintf("sum(increase(%s[%s]))", metric, window)
// Result: Single aggregated metric

// With grouping: [handler]
expr := fmt.Sprintf("sum by (handler) (increase(%s[%s]))", metric, window)
// Result: Multiple metrics, one per handler value
```

**SLO Instantiation:**
- Controller watches for ServiceLevelObjective CRDs
- Processes `grouping` field to determine instance creation
- Creates separate SLO objects for each unique label value
- Each instance gets its own recording rules

**Main Page vs Detail Page:**
- **Main Page:** Queries all SLO instances, displays each separately
- **Detail Page:** Queries specific SLO instance by labels
- **No Mismatch:** Both use same recording rules, just different label selectors

---

### NaN Issue Architecture

**Recording Rule Structure:**

```yaml
# Burn rate recording rule (generated by Pyrra)
- record: prometheus_http_requests:burnrate3m
  expr: |
    sum(rate(prometheus_http_requests_total{code=~"5..",handler=~"/api.*"}[3m]))
    / sum(rate(prometheus_http_requests_total{handler=~"/api.*"}[3m]))
  labels:
    slo: test-regex-no-grouping
```

**When No Errors Exist:**
- Numerator query returns empty (no 5xx errors)
- Denominator query returns value (traffic exists)
- Division: empty / value = empty
- Recording rule produces no time series
- UI query for recording rule returns empty
- UI displays empty as NaN

**Why Alerts Still Work:**

```promql
# Alert rule expression
prometheus_http_requests:burnrate3m{slo="test"} > 0.14

# When recording rule is empty:
[] > 0.14 → false (alert doesn't fire)

# This is correct behavior - no errors = no alert
```

---

## Dynamic Burn Rate Impact Analysis

### Does Dynamic Burn Rate Exacerbate Issues?

**Issue #1 (Grouping):**
- ❌ No impact - grouping behavior is identical for static and dynamic
- Dynamic burn rate only affects alert threshold calculation
- SLO instantiation logic unchanged

**Issue #2 (NaN):**
- ❌ No impact - NaN occurs with both static and dynamic when no errors
- Dynamic threshold calculation doesn't affect burn rate recording rules
- Both use same recording rule structure

**Conclusion:** Dynamic burn rate feature does NOT introduce or worsen these issues.

---

## Comparison: Static vs Dynamic Behavior

### Recording Rules (Identical Structure)

**Static Burn Rate:**
```promql
sum(rate(metric{code=~"5.."}[3m])) / sum(rate(metric[3m]))
```

**Dynamic Burn Rate:**
```promql
sum(rate(metric{code=~"5.."}[3m])) / sum(rate(metric[3m]))
```

**Alert Rules (Different Thresholds Only):**

**Static:**
```promql
burnrate3m{slo="test"} > (14 * (1 - 0.99))
```

**Dynamic:**
```promql
burnrate3m{slo="test"} > scalar((sum(metric:increase30d{slo="test"}) / sum(increase(metric[3m]))) * (1/48) * (1 - 0.99))
```

**Key Insight:** Only alert threshold calculation differs. Recording rules and SLO instantiation are identical.

---

## Architectural Questions Answered

### Q1: How does grouping field affect SLO instantiation?

**Answer:** Grouping creates multiple SLO instances, one per unique label value.

**Mechanism:**
1. Pyrra reads `grouping: [handler]` from YAML
2. Queries Prometheus for unique handler values
3. Creates separate SLO instance for each value
4. Each instance has its own recording rules with label selector

**Example:**
```yaml
grouping: [handler]
metric: prometheus_http_requests_total{handler=~"/api.*"}
```

**Results in:**
- SLO instance 1: handler="/api/v1/query"
- SLO instance 2: handler="/api/v1/query_range"
- SLO instance 3: handler="/api/v1/notifications/live"
- ... (one per matching handler)

---

### Q2: Are recording rules per-group or aggregated?

**Answer:** Recording rules are per-group when grouping is specified.

**With Grouping:**
```promql
sum by (handler) (increase(prometheus_http_requests_total{handler=~"/api.*"}[2w]))
```
- Creates multiple time series, one per handler
- Each SLO instance queries its specific handler value

**Without Grouping:**
```promql
sum(increase(prometheus_http_requests_total{handler=~"/api.*"}[2w]))
```
- Creates single aggregated time series
- Single SLO instance queries aggregated metric

---

### Q3: Where does main page vs detail page calculation diverge?

**Answer:** They don't diverge - both use same recording rules with different label selectors.

**Main Page Query:**
```promql
# Lists all SLO instances
sum by (slo, handler) (prometheus_http_requests:increase2w{handler=~"/api.*"})
```

**Detail Page Query:**
```promql
# Queries specific SLO instance
sum(prometheus_http_requests:increase2w{slo="test-regex-static",handler="/api/v1/query"})
```

**No Mismatch:** Both queries use same recording rules, just different label filters.

---

### Q4: Does dynamic burn rate logic exacerbate the issue?

**Answer:** No - dynamic burn rate only affects alert threshold calculation, not SLO instantiation or recording rules.

**What Dynamic Burn Rate Changes:**
- Alert threshold calculation (uses traffic-aware formula)
- Alert rule expressions (includes dynamic threshold)

**What Dynamic Burn Rate Does NOT Change:**
- SLO instantiation logic
- Recording rule generation
- Grouping behavior
- UI data display logic

---

### Q5: What is the intended relationship between SLO YAML and SLO instances?

**Answer:** Unclear from upstream documentation, but behavior suggests:

**Current Design:**
- One YAML can produce multiple SLO instances (with grouping)
- Allows per-endpoint SLO tracking
- Each instance is independent

**Alternative Design (Not Implemented):**
- One YAML = One SLO (always aggregated)
- Simpler, more predictable
- Users create multiple YAMLs for per-endpoint tracking

**Recommendation:** Clarify intended design with upstream maintainers.

---

## Conclusions

### Issue #1: Grouping Behavior

**Status:** ✅ Understood - Intentional design (or long-standing limitation)

**Root Cause:**
- Pyrra creates multiple SLO instances for grouped labels
- Recording rules correctly scoped per-group
- Detail pages work as designed

**Not a Bug Because:**
- Recording rules are correct
- Detail pages show correct data
- Behavior is consistent

**User Confusion Because:**
- Documentation doesn't explain this behavior
- Users expect one YAML = one SLO
- Can create dozens of SLOs unexpectedly

**Recommendation:** Document behavior clearly, provide guidance on when to use grouping.

---

### Issue #2: NaN Display

**Status:** ✅ Understood - Prometheus empty result handling

**Root Cause:**
- Prometheus returns empty (not 0) for non-existent metrics
- UI doesn't handle empty results gracefully
- Shows NaN instead of 0

**Is a Bug Because:**
- Poor user experience (confusing)
- Should show 0 or "No errors" instead of NaN
- Other parts of UI handle this correctly (availability tiles)

**Impact:**
- Cosmetic only - alerts work correctly
- Affects all SLOs when no errors
- Universal issue, not specific to regex selectors

**Recommendation:** Fix UI to show 0 instead of NaN (likely already fixed in feature branch).

---

## Recommendations

### For Task 8.4.3 (Solution Implementation)

**Choose Option B: Document Limitation**

**Actions:**
1. ✅ Document grouping behavior in KNOWN_LIMITATIONS.md (DONE)
2. ✅ Document NaN issue in KNOWN_LIMITATIONS.md (DONE)
3. ⏭️ Verify feature branch fixes NaN issue
4. ⏭️ Add user guidance to examples/README.md
5. ⏭️ Update production docs with best practices

**No Code Changes Required:**
- Grouping behavior is by design
- NaN fix likely already in feature branch
- No regressions to fix

---

### For Upstream Contribution

**Include in PR:**
1. NaN fix (if present in feature branch)
2. Documentation improvements for grouping behavior
3. Best practices guidance for regex selectors

**Discuss with Upstream:**
1. Is grouping behavior intentional?
2. Should one YAML = one SLO?
3. Gather maintainer feedback on design philosophy

---

## References

- **Upstream Comparison:** `.dev-docs/UPSTREAM_COMPARISON_REGEX_SELECTORS.md`
- **Known Limitations:** `.dev-docs/KNOWN_LIMITATIONS.md`
- **Task 8.4.1 Summary:** `.dev-docs/TASK_8.4.1_COMPLETION_SUMMARY.md`
- **Code Locations:** `slo/rules.go`, `slo/slo.go`, `kubernetes/controllers/`

---

**Analysis Completed By:** AI Development Session  
**Date:** 2025-10-14  
**Status:** ✅ Ready for Task 8.4.3 (Solution Implementation)
